# -*- coding: utf-8 -*-
"""ewaste_end_to_end.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P0wRavTH6Pg9ZUmzDa_bwJe7y3LJLROT

# **AI-Based E-Waste Identification Using Camera Input**

Module: M516 â€“ Business Project in Big Data & AI  
Student Name: Dinakar Vennalakanti and Sai Pavan Kalyan Amalanadhuni
Student ID: GH1038092 and GH1034980 
Date: 18th December 2025

## **Notebook 01: Problem and Data Understanding**

## Background and Motivation

Electronic waste, commonly known as e-waste, represents one of the fastest growing waste streams globally. Items such as batteries, circuit boards, mobile phones, and chargers are frequently discarded despite containing valuable and hazardous materials.

Recycling centres often rely on manual sorting processes, which are labour-intensive, time-consuming, and prone to error. The lack of automated identification systems limits the efficiency of recycling operations and contributes to low recovery rates.

Advances in computer vision and deep learning provide an opportunity to automatically identify e-waste items using visual input. This project explores whether a locally deployed AI system can classify e-waste items using camera input as a proof-of-concept for sustainability-focused decision support.

## Problem Statement

Recycling facilities lack accessible AI-based tools that can automatically identify types of electronic waste using camera input.

This limitation reduces the ability to:
- Understand e-waste composition
- Improve sorting efficiency
- Support sustainable recycling decisions

This project investigates the feasibility of using image classification techniques to identify e-waste items from camera input in a local environment.


## Project Objectives

The objectives of this project are:

1. To analyse and understand publicly available e-waste image datasets.
2. To perform exploratory data analysis to assess dataset quality and structure.
3. To train and evaluate an image classification model for e-waste identification.
4. To deploy the trained model locally and demonstrate real-time classification using a webcam.
5. To critically evaluate the limitations and sustainability implications of the approach.

## Scope and Assumptions

The project operates under the following assumptions:

- Only one primary object is visible in the camera frame at a time.
- The object is placed close to the camera and reasonably well-lit.
- The task is image classification, not object detection.
- Model inference is performed entirely on a local machine.
- Cloud deployment and industrial-scale integration are outside the scope.

The system is designed as a proof-of-concept rather than a production-ready solution.

## Dataset Overview

### Kaggle E-Waste Image Dataset

Source:
https://www.kaggle.com/datasets/akshat103/e-waste-image-dataset

This dataset contains labelled images of common electronic waste categories and is used as the primary dataset for model training and validation.

### Roboflow E-Waste Dataset

Source:
https://universe.roboflow.com/electronic-waste-detection/e-waste-dataset-r0ojc

This dataset is used only for qualitative evaluation and webcam inference testing. It is not used during training to avoid data leakage and annotation inconsistencies.
"""

from google.colab import files
files.upload()

import os
os.makedirs("/root/.kaggle", exist_ok=True)

!mv kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json
!ls -la /root/.kaggle

os.makedirs("/content/data", exist_ok=True)

!kaggle datasets download -d akshat103/e-waste-image-dataset -p /content/data

import zipfile

zip_path = "/content/data/e-waste-image-dataset.zip"
extract_path = "/content/data/e_waste"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Verify extraction
!ls /content/data/e_waste

BASE_DATA_PATH = "/content/data/e_waste/modified-dataset"

TRAIN_PATH = f"{BASE_DATA_PATH}/train"
VAL_PATH   = f"{BASE_DATA_PATH}/val"
TEST_PATH  = f"{BASE_DATA_PATH}/test"

print("Base dataset path:", BASE_DATA_PATH)

import os

for split_name, split_path in {
    "TRAIN": TRAIN_PATH,
    "VALIDATION": VAL_PATH,
    "TEST": TEST_PATH
}.items():

    assert os.path.exists(split_path), f"{split_name} path not found"

    classes = [
        d for d in os.listdir(split_path)
        if os.path.isdir(os.path.join(split_path, d))
    ]

    print(f"\n{split_name} SET")
    print("Number of classes:", len(classes))
    for cls in sorted(classes):
        print("-", cls)

!pip install ultralytics roboflow

from roboflow import Roboflow

rf = Roboflow(api_key="gA6HkRKvzTjJXwHYnMFc")
project = rf.workspace("electronic-waste-detection").project("e-waste-dataset-r0ojc")
dataset = project.version(1).download("yolov8")

def count_images(path):
    return {
        cls: len(os.listdir(os.path.join(path, cls)))
        for cls in os.listdir(path)
    }

df_counts = pd.DataFrame({
    "Train": count_images(TRAIN_PATH),
    "Validation": count_images(VAL_PATH),
    "Test": count_images(TEST_PATH)
})

df_counts

from ultralytics import YOLO

# Load YOLOv8 nano model (fast + CPU friendly)
model = YOLO("yolov8n.pt")

# Train
model.train(
    data="E-Waste-Dataset-1/data.yaml",  # update folder name if needed
    epochs=5,
    imgsz=416,
    batch=8,
    freeze=10,
    val=False
)

!pip install kaggle tensorflow pillow matplotlib seaborn scikit-learn

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image

sns.set(style="whitegrid")

df_counts.plot(kind="bar", figsize=(12,6))
plt.title("Image Distribution Across Dataset Splits")
plt.ylabel("Image Count")
plt.xticks(rotation=45)
plt.show()

def show_samples(class_name, n=4):
    imgs = os.listdir(os.path.join(TRAIN_PATH, class_name))[:n]
    plt.figure(figsize=(n*2,2))
    for i, img in enumerate(imgs):
        im = Image.open(os.path.join(TRAIN_PATH, class_name, img))
        plt.subplot(1,n,i+1)
        plt.imshow(im)
        plt.axis("off")
    plt.show()

for cls in os.listdir(TRAIN_PATH):
    show_samples(cls)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras import Input

IMG_SIZE = (224,224)
BATCH_SIZE = 32

train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.1,
    horizontal_flip=True
)

val_test_gen = ImageDataGenerator(rescale=1./255)

train_data = train_gen.flow_from_directory(
    TRAIN_PATH,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

val_data = val_test_gen.flow_from_directory(
    VAL_PATH,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False
)

test_data = val_test_gen.flow_from_directory(
    TEST_PATH,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False
)

input_tensor = Input(shape=(224, 224, 3))

base_model = MobileNetV2(
    weights="imagenet",
    include_top=False,
    input_tensor=input_tensor
)
base_model.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation="relu")(x)
output = Dense(train_data.num_classes, activation="softmax")(x)

model = Model(inputs=input_tensor, outputs=output)

model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=10
)

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

preds = model.predict(test_data)
y_pred = np.argmax(preds, axis=1)
y_true = test_data.classes
labels = list(test_data.class_indices.keys())

print(classification_report(y_true, y_pred, target_names=labels))